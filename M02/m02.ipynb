{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.kernel.restart()",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "\n",
    "def restart_kernel():\n",
    "    display(Javascript('IPython.notebook.kernel.restart()'))\n",
    "\n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Conv1D, dot, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization  # Modificación realizada aquí\n",
    "from keras.layers import MaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Lambda, Flatten, Dense, Dropout\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import os, random, pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy.random as rng\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2556\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, name=None, dtype=None):\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "\n",
    "def initialize_bias(shape, name=None, dtype=None):\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_layer(encoded_l, encoded_r, lyr_name='cos'):\n",
    "\n",
    "    if lyr_name == 'L1':\n",
    "        # Add a customized layer to compute the absolute difference between the encodings\n",
    "        L1_layer = Lambda(lambda tensors: tf.keras.backend.abs(tensors[0] - tensors[1]))\n",
    "        L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "        add_dens =  Dense(512, activation='relu', bias_initializer=initialize_bias)(L1_distance)\n",
    "        # drp_lyr = Dropout(0.25)(add_dens)\n",
    "        # xx =  Dense(128, activation='relu', bias_initializer=initialize_bias)(add_dens)\n",
    "        # prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(L1_distance)\n",
    "        prediction = Dense(1, activation='sigmoid')(xx)\n",
    "\n",
    "    elif lyr_name == 'L2':\n",
    "\n",
    "        # Write L2 here\n",
    "        L2_layer = Lambda(lambda tensors: (tensors[0] - tensors[1]) ** 2 / (tensors[0] + tensors[1]))\n",
    "        L2_distance = L2_layer([encoded_l, encoded_r])\n",
    "        add_dens = Dense(512, activation='relu', bias_initializer=initialize_bias)(L2_distance)\n",
    "        drp_lyr = Dropout(0.25)(add_dens)\n",
    "        # xx =  Dense(128, activation='relu', bias_initializer=initialize_bias)(drp_lyr)\n",
    "        # drp_lyr2 = Dropout(0.25)(xx)\n",
    "        # x =  Dense(64, activation='relu', bias_initializer=initialize_bias)(xx)\n",
    "        prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(drp_lyr)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Add cosine similarity function\n",
    "        cos_layer= Lambda(lambda tensors: K.sum(tensors[0] * tensors[1], axis=-1, keepdims=True)/\n",
    "                                          tf.keras.backend.l2_normalize(tensors[0])*tf.keras.backend.l2_normalize(tensors[1]))\n",
    "        cos_distance = cos_layer([encoded_l, encoded_r])\n",
    "        prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(cos_distance)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_pairs(sample_size=1000):\n",
    "    # Inicializar listas para almacenar los pares y las etiquetas\n",
    "    pairs = [[], []]\n",
    "    targets = []\n",
    "\n",
    "    # Generar índices aleatorios para los pares\n",
    "    indices = np.random.choice(len(y_test), size=(sample_size, 2), replace=True)\n",
    "\n",
    "    for idx1, idx2 in indices:\n",
    "        pairs[0].append(x_test.iloc[idx1].values.reshape(genes_len, 1))\n",
    "        pairs[1].append(x_test.iloc[idx2].values.reshape(genes_len, 1))\n",
    "        targets.append(int(y_test[idx1] == y_test[idx2]))\n",
    "\n",
    "    # Convertir las listas en arrays numpy\n",
    "    pairs[0] = np.array(pairs[0])\n",
    "    pairs[1] = np.array(pairs[1])\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return pairs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    # Define the inputs for the two samples\n",
    "    left_input = Input(shape=input_shape)\n",
    "    right_input = Input(shape=input_shape)\n",
    "\n",
    "    # Shared Dense network with Flatten layer\n",
    "    shared_dense = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Generate the encodings for the two inputs\n",
    "    encoded_l = shared_dense(left_input)\n",
    "    encoded_r = shared_dense(right_input)\n",
    "\n",
    "    # Compute the absolute difference between the encodings\n",
    "    L1_distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded_l, encoded_r])\n",
    "\n",
    "    # Add a final Dense layer with sigmoid activation\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Define the Siamese network model\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    return siamese_net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_siamese_model(input_shape):\\n    # Definir las entradas para las dos muestras\\n    left_input = Input(shape=input_shape)\\n    right_input = Input(shape=input_shape)\\n    \\n    # Red Neuronal Simple\\n    model = Sequential()\\n    model.add(Flatten(input_shape=input_shape))\\n    model.add(Dense(64, activation='relu'))\\n    model.add(Dense(32, activation='relu'))\\n    model.add(Dense(16, activation='relu'))\\n\\n    # Generar las representaciones (embeddings) para las dos entradas\\n    encoded_l = model(left_input)\\n    encoded_r = model(right_input)\\n    \\n    # Cálculo de la distancia L1 entre las representaciones\\n    L1_distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded_l, encoded_r])\\n    \\n    # Capa de decisión final\\n    prediction = Dense(1, activation='sigmoid')(L1_distance)\\n    \\n    # Definir el modelo\\n    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\\n    return siamese_net\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_siamese_model(input_shape):\n",
    "    # Definir las entradas para las dos muestras\n",
    "    left_input = Input(shape=input_shape)\n",
    "    right_input = Input(shape=input_shape)\n",
    "    \n",
    "    # Red Neuronal Simple\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "\n",
    "    # Generar las representaciones (embeddings) para las dos entradas\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Cálculo de la distancia L1 entre las representaciones\n",
    "    L1_distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded_l, encoded_r])\n",
    "    \n",
    "    # Capa de decisión final\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # Definir el modelo\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    return siamese_net\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model):\n",
    "    pairs_test, targets_test = get_test_pairs()\n",
    "    predictions = model.predict(pairs_test)\n",
    "    predictions = predictions.ravel()\n",
    "\n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(targets_test, predictions > 0.5)\n",
    "    roc_auc = roc_auc_score(targets_test, predictions)\n",
    "\n",
    "    print(f\"Precisión en el conjunto de prueba: {acc:.2f}\")\n",
    "    print(f\"ROC-AUC en el conjunto de prueba: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_siamese_model(input_shape):\\n    # Definir las entradas para las dos muestras\\n    left_input = Input(input_shape)\\n    right_input = Input(input_shape)\\n\\n    # Red Neuronal Convolucional Compartida\\n    model = Sequential()\\n    model.add(Conv1D(filters=64, kernel_size=10, activation='relu', input_shape=input_shape))\\n    model.add(MaxPooling1D(pool_size=2))\\n    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\\n    model.add(MaxPooling1D(pool_size=2))\\n    model.add(Flatten())\\n    model.add(Dense(128, activation='sigmoid'))\\n\\n    # Generar las representaciones (embeddings) para las dos entradas\\n    encoded_l = model(left_input)\\n    encoded_r = model(right_input)\\n\\n    # Cálculo de la distancia L1 entre las representaciones\\n    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\\n    L1_distance = L1_layer([encoded_l, encoded_r])\\n\\n    # Capa de decisión final\\n    prediction = Dense(1, activation='sigmoid')(L1_distance)\\n\\n    # Definir el modelo\\n    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\\n    return siamese_net\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comentario\n",
    "\"\"\"\n",
    "def get_siamese_model(input_shape):\n",
    "    # Definir las entradas para las dos muestras\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "    # Red Neuronal Convolucional Compartida\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "\n",
    "    # Generar las representaciones (embeddings) para las dos entradas\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    # Cálculo de la distancia L1 entre las representaciones\n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    # Capa de decisión final\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Definir el modelo\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    return siamese_net\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"tcga_common_data_peru.csv\"\n",
    "TCGA_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OR4F5</th>\n",
       "      <th>SAMD11</th>\n",
       "      <th>KLHL17</th>\n",
       "      <th>PLEKHN1</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>AGRN</th>\n",
       "      <th>TTLL10</th>\n",
       "      <th>B3GALT6</th>\n",
       "      <th>SCNN1D</th>\n",
       "      <th>PUSL1</th>\n",
       "      <th>...</th>\n",
       "      <th>UTY</th>\n",
       "      <th>VCY</th>\n",
       "      <th>CDY2B</th>\n",
       "      <th>HSFY2</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>RBMY1E</th>\n",
       "      <th>PRY2</th>\n",
       "      <th>CDY1B</th>\n",
       "      <th>DAZ3</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254728</td>\n",
       "      <td>3.199541</td>\n",
       "      <td>2.841540</td>\n",
       "      <td>1.753755</td>\n",
       "      <td>3.672659</td>\n",
       "      <td>3.421789</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>3.293547</td>\n",
       "      <td>2.954878</td>\n",
       "      <td>3.032272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201748</td>\n",
       "      <td>0.297927</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.224009</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.254728</td>\n",
       "      <td>3.016719</td>\n",
       "      <td>2.546730</td>\n",
       "      <td>1.320451</td>\n",
       "      <td>3.441749</td>\n",
       "      <td>3.403565</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>3.091509</td>\n",
       "      <td>2.645649</td>\n",
       "      <td>2.890673</td>\n",
       "      <td>...</td>\n",
       "      <td>2.971343</td>\n",
       "      <td>0.297927</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>3.204342</td>\n",
       "      <td>0.224009</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.254728</td>\n",
       "      <td>2.029367</td>\n",
       "      <td>2.708073</td>\n",
       "      <td>0.668227</td>\n",
       "      <td>3.139925</td>\n",
       "      <td>3.555979</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>2.988379</td>\n",
       "      <td>2.381621</td>\n",
       "      <td>2.917346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.885337</td>\n",
       "      <td>0.297927</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>2.765418</td>\n",
       "      <td>0.224009</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254728</td>\n",
       "      <td>0.244785</td>\n",
       "      <td>2.733089</td>\n",
       "      <td>1.884489</td>\n",
       "      <td>3.260723</td>\n",
       "      <td>3.374143</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>3.157854</td>\n",
       "      <td>2.823415</td>\n",
       "      <td>3.011421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201748</td>\n",
       "      <td>0.297927</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.224009</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254728</td>\n",
       "      <td>2.470030</td>\n",
       "      <td>2.674949</td>\n",
       "      <td>2.086111</td>\n",
       "      <td>3.183263</td>\n",
       "      <td>3.496209</td>\n",
       "      <td>0.358315</td>\n",
       "      <td>3.091784</td>\n",
       "      <td>2.899107</td>\n",
       "      <td>2.797831</td>\n",
       "      <td>...</td>\n",
       "      <td>2.601769</td>\n",
       "      <td>0.297927</td>\n",
       "      <td>0.227856</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>2.904073</td>\n",
       "      <td>0.224009</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>0.202187</td>\n",
       "      <td>0.225862</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16076 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OR4F5    SAMD11    KLHL17   PLEKHN1     ISG15      AGRN    TTLL10  \\\n",
       "0  0.254728  3.199541  2.841540  1.753755  3.672659  3.421789  0.358315   \n",
       "1  0.254728  3.016719  2.546730  1.320451  3.441749  3.403565  0.358315   \n",
       "2  0.254728  2.029367  2.708073  0.668227  3.139925  3.555979  0.358315   \n",
       "3  0.254728  0.244785  2.733089  1.884489  3.260723  3.374143  0.358315   \n",
       "4  0.254728  2.470030  2.674949  2.086111  3.183263  3.496209  0.358315   \n",
       "\n",
       "    B3GALT6    SCNN1D     PUSL1  ...       UTY       VCY     CDY2B     HSFY2  \\\n",
       "0  3.293547  2.954878  3.032272  ...  0.201748  0.297927  0.227856  0.168309   \n",
       "1  3.091509  2.645649  2.890673  ...  2.971343  0.297927  0.227856  0.168309   \n",
       "2  2.988379  2.381621  2.917346  ...  2.885337  0.297927  0.227856  0.168309   \n",
       "3  3.157854  2.823415  3.011421  ...  0.201748  0.297927  0.227856  0.168309   \n",
       "4  3.091784  2.899107  2.797831  ...  2.601769  0.297927  0.227856  0.168309   \n",
       "\n",
       "      KDM5D    RBMY1E      PRY2     CDY1B      DAZ3   labels  \n",
       "0  0.198074  0.224009  0.312519  0.202187  0.225862   cancer  \n",
       "1  3.204342  0.224009  0.312519  0.202187  0.225862  healthy  \n",
       "2  2.765418  0.224009  0.312519  0.202187  0.225862   cancer  \n",
       "3  0.198074  0.224009  0.312519  0.202187  0.225862   cancer  \n",
       "4  2.904073  0.224009  0.312519  0.202187  0.225862   cancer  \n",
       "\n",
       "[5 rows x 16076 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCGA_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecciona datos de rna_seq_data_common donde labels es cancer\n",
    "TCGA_data__cancer = TCGA_data[TCGA_data.labels == 'cancer'].sample(100, random_state=seed)\n",
    "#selecciona 50 datos de rna_seq_data_common donde labels es healthy\n",
    "TCGA_data_healthy = TCGA_data[TCGA_data.labels == 'healthy'].sample(50, random_state=seed)\n",
    "#concatena los datos\n",
    "TCGA_data_balanced = pd.concat([TCGA_data__cancer, TCGA_data_healthy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = TCGA_data_balanced['labels'].values\n",
    "y = np.where(TCGA_data_balanced['labels'].values == 'cancer', 1, 0)\n",
    "X = TCGA_data_balanced.drop('labels', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_len = X.shape[1]\n",
    "input_shape = (genes_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Configurar K-Fold Cross-Validation\n",
    "n_splits = 10\n",
    "#kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/sespinoza/objetivo1/CancerSiamese/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "batch_size = 64\n",
    "n_iter = 2000  # Puedes ajustar este número\n",
    "evaluate_every = 100  # Frecuencia de evaluación\n",
    "N_way = 1\n",
    "n_val = 100  # Número de tareas de one-shot para validar\n",
    "best_acc = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 16075)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  50]\n",
      " [  1 100]]\n"
     ]
    }
   ],
   "source": [
    "#cuenta la cantidad de valores por clase en y\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "#impresión de la cantidad de valores por clase\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#tf.config.optimizer.set_jit(False)  # Desactiva la compilación JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el pliegue 1/10\n",
      "NaN en X_train_final: False\n",
      "Inf en X_train_final: False\n",
      "Shape of inputs[0]: (64, 25, 1)\n",
      "Shape of inputs[1]: (64, 25, 1)\n",
      "Shape of targets: (64,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SGD' object has no attribute 'get_updates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3271006/1151991500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of inputs[1]:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of targets:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training_v1.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2282\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m                     \u001b[0;31m# Training updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                     updates = self.optimizer.get_updates(\n\u001b[0m\u001b[1;32m   2285\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'get_updates'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(f\"\\nEntrenando el pliegue {fold}/{n_splits}\")\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba para este pliegue\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Verificar la presencia de ambas clases en y_train\n",
    "    unique_classes = np.unique(y_train)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Advertencia: Sólo una clase presente en el pliegue {fold}. Saltando este pliegue.\")\n",
    "        continue  # Salta al siguiente pliegue\n",
    "\n",
    "    # Convertir los datos a DataFrames para mantener consistencia con el código original\n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "    # Calcular MI en el conjunto de entrenamiento\n",
    "    MI = mutual_info_classif(X_train_df, y_train)\n",
    "    n_features_mi = 5000  # Puedes ajustar este número\n",
    "    selected_scores_indices = np.argsort(MI)[::-1][:n_features_mi]\n",
    "\n",
    "    # Seleccionar las características en base a MI\n",
    "    X_train_mi = X_train_df.iloc[:, selected_scores_indices]\n",
    "    X_test_mi = X_test_df.iloc[:, selected_scores_indices]\n",
    "\n",
    "    # **Paso 2: Selección de Características con RFE**\n",
    "    estimator = linear_model.LogisticRegression(max_iter=700, solver='liblinear')\n",
    "    n_features_rfe = 25  # Puedes ajustar este número\n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=n_features_rfe, step=1)\n",
    "    rfe.fit(X_train_mi, y_train)\n",
    "\n",
    "    # Transformar los datos de entrenamiento y prueba\n",
    "    X_train_final = rfe.transform(X_train_mi)\n",
    "    X_test_final = rfe.transform(X_test_mi)\n",
    "\n",
    "    print('NaN en X_train_final:', np.isnan(X_train_final).any())\n",
    "    print('Inf en X_train_final:', np.isinf(X_train_final).any())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_final = scaler.fit_transform(X_train_final)\n",
    "    X_test_final = scaler.transform(X_test_final)\n",
    "\n",
    "    X_train_final = X_train_final.astype('float32')\n",
    "    X_test_final = X_test_final.astype('float32')\n",
    "\n",
    "    # Actualizar genes_len e input_shape\n",
    "    genes_len = X_train_final.shape[1]\n",
    "    input_shape = (genes_len, 1)\n",
    "\n",
    "    x_train = pd.DataFrame(X_train_final)\n",
    "    x_test = pd.DataFrame(X_test_final)\n",
    "\n",
    "    # Definir el modelo\n",
    "    model = get_siamese_model(input_shape)\n",
    "    # Update the learning rate to a reasonable value\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Definir las funciones get_batch y get_test_pairs utilizando los datos del pliegue actual\n",
    "\n",
    "    def get_batch(batch_size):\n",
    "        # Función para generar un lote de entrenamiento\n",
    "        half_batch = batch_size // 2\n",
    "        pairs = [np.zeros((batch_size, genes_len, 1)) for _ in range(2)]\n",
    "        targets = np.zeros((batch_size,))\n",
    "        targets[half_batch:] = 1  # La segunda mitad son pares de la misma clase\n",
    "\n",
    "        # Obtener índices de muestras de cada clase\n",
    "        cancer_indices = np.where(y_train == 1)[0]\n",
    "        non_cancer_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "        for i in range(half_batch):\n",
    "            # Pares de clases diferentes (una cáncer, otra no cáncer)\n",
    "            idx_cancer = np.random.choice(cancer_indices)\n",
    "            idx_non_cancer = np.random.choice(non_cancer_indices)\n",
    "\n",
    "            pairs[0][i, :, 0] = x_train.iloc[idx_cancer].values\n",
    "            pairs[1][i, :, 0] = x_train.iloc[idx_non_cancer].values\n",
    "\n",
    "        for i in range(half_batch, batch_size):\n",
    "            # Pares de la misma clase\n",
    "            if np.random.rand() < 0.5:\n",
    "                # Ambos cáncer\n",
    "                if len(cancer_indices) >= 2:\n",
    "                    idx1, idx2 = np.random.choice(cancer_indices, size=2, replace=False)\n",
    "                else:\n",
    "                    idx1 = idx2 = np.random.choice(cancer_indices)\n",
    "            else:\n",
    "                # Ambos no cáncer\n",
    "                if len(non_cancer_indices) >= 2:\n",
    "                    idx1, idx2 = np.random.choice(non_cancer_indices, size=2, replace=False)\n",
    "                else:\n",
    "                    idx1 = idx2 = np.random.choice(non_cancer_indices)\n",
    "\n",
    "            pairs[0][i, :, 0] = x_train.iloc[idx1].values\n",
    "            pairs[1][i, :, 0] = x_train.iloc[idx2].values\n",
    "\n",
    "        return pairs, targets\n",
    "\n",
    "    # Definir las funciones get_test_pairs y evaluate_model como antes...\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    n_iter = 2000  # Ajusta este número según tus recursos\n",
    "    batch_size = 64\n",
    "    evaluate_every = 100\n",
    "    best_acc = -1\n",
    "\n",
    "    for i in range(1, n_iter + 1):\n",
    "        inputs, targets = get_batch(batch_size)\n",
    "        print('Shape of inputs[0]:', inputs[0].shape)\n",
    "        print('Shape of inputs[1]:', inputs[1].shape)\n",
    "        print('Shape of targets:', targets.shape)\n",
    "        loss, acc = model.train_on_batch(inputs, targets)\n",
    "        \n",
    "\n",
    "        if i % evaluate_every == 0:\n",
    "            print(f\"Iteración {i}, pérdida de entrenamiento: {loss:.4f}, precisión: {acc:.4f}\")\n",
    "            evaluate_model(model)\n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el pliegue 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3148403/987282472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# Reiniciar la mejor precisión para cada pliegue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3148403/987282472.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Pares de clases diferentes (una cáncer, otra no cáncer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0midx_cancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcancer_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0midx_non_cancer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_cancer_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "fold=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"\\nEntrenando el pliegue {fold}/{n_splits}\")\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba para este pliegue\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Convertir los datos a DataFrames para mantener consistencia con el código original\n",
    "    x_train = pd.DataFrame(X_train, index=y_train)\n",
    "    x_test = pd.DataFrame(X_test, index=y_test)\n",
    "\n",
    "    # Crear diccionarios de índices por clase\n",
    "    def indices_save(labels):\n",
    "        class_dic = {}\n",
    "        for k in np.unique(labels):\n",
    "            indices = [i for i, label in enumerate(labels) if label == k]\n",
    "            class_dic[k] = indices\n",
    "        return class_dic\n",
    "\n",
    "    class_train_ind = indices_save(y_train)\n",
    "    class_test_ind = indices_save(y_test)\n",
    "\n",
    "    # Definir las funciones get_batch y make_oneshot_task utilizando los datos del pliegue actual\n",
    "\n",
    "    def get_batch(batch_size):\n",
    "        # Función para generar un lote de entrenamiento\n",
    "        half_batch = batch_size // 2\n",
    "        pairs = [np.zeros((batch_size, genes_len, 1)) for _ in range(2)]\n",
    "        targets = np.zeros((batch_size,))\n",
    "        targets[half_batch:] = 1  # La segunda mitad son pares de la misma clase\n",
    "\n",
    "        # Obtener índices de muestras de cada clase\n",
    "        cancer_indices = np.where(y_train == 1)[0]\n",
    "        non_cancer_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "        for i in range(half_batch):\n",
    "            # Pares de clases diferentes (una cáncer, otra no cáncer)\n",
    "            idx_cancer = np.random.choice(cancer_indices)\n",
    "            idx_non_cancer = np.random.choice(non_cancer_indices)\n",
    "\n",
    "            pairs[0][i, :, 0] = x_train.iloc[idx_cancer].values\n",
    "            pairs[1][i, :, 0] = x_train.iloc[idx_non_cancer].values\n",
    "\n",
    "        for i in range(half_batch, batch_size):\n",
    "            # Pares de la misma clase\n",
    "            if np.random.rand() < 0.5:\n",
    "                # Ambos cáncer\n",
    "                idx1, idx2 = np.random.choice(cancer_indices, size=2, replace=False)\n",
    "            else:\n",
    "                # Ambos no cáncer\n",
    "                idx1, idx2 = np.random.choice(non_cancer_indices, size=2, replace=False)\n",
    "\n",
    "            pairs[0][i, :, 0] = x_train.iloc[idx1].values\n",
    "            pairs[1][i, :, 0] = x_train.iloc[idx2].values\n",
    "\n",
    "        return pairs, targets\n",
    "\n",
    "\n",
    "    def make_oneshot_task():\n",
    "        # Seleccionar una muestra de prueba\n",
    "        idx = np.random.choice(len(y_test))\n",
    "        test_sample = x_test.iloc[idx].values.reshape(1, genes_len, 1)\n",
    "        test_label = y_test[idx]\n",
    "\n",
    "        # Crear un conjunto de soporte (support set)\n",
    "        support_set = []\n",
    "        targets = []\n",
    "        for _ in range(N_way):\n",
    "            if np.random.rand() < 0.5:\n",
    "                # Muestra de la misma clase\n",
    "                same_class_indices = np.where(y_test == test_label)[0]\n",
    "                support_idx = np.random.choice(same_class_indices)\n",
    "                targets.append(1)\n",
    "            else:\n",
    "                # Muestra de clase diferente\n",
    "                different_class_indices = np.where(y_test != test_label)[0]\n",
    "                support_idx = np.random.choice(different_class_indices)\n",
    "                targets.append(0)\n",
    "            support_sample = x_test.iloc[support_idx].values.reshape(1, genes_len, 1)\n",
    "            support_set.append(support_sample)\n",
    "\n",
    "        # Convertir a arrays numpy\n",
    "        support_set = np.vstack(support_set)\n",
    "        test_samples = np.vstack([test_sample] * N_way)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        pairs = [test_samples, support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "\n",
    "\n",
    "    def test_oneshot(model, N_way, n_trials):\n",
    "        n_correct = 0\n",
    "        for _ in range(n_trials):\n",
    "            inputs, targets = make_oneshot_task(N_way)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct += 1\n",
    "        return 100.0 * n_correct / n_trials\n",
    "\n",
    "    # Definir el modelo\n",
    "    # Definir el modelo\n",
    "    model = get_siamese_model(input_shape)\n",
    "    optimizer = Adam(lr=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    best_acc = -1  # Reiniciar la mejor precisión para cada pliegue\n",
    "    for i in range(1, n_iter + 1):\n",
    "        inputs, targets = get_batch(batch_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "\n",
    "        if i % evaluate_every == 0:\n",
    "            print(f\"Iteración {i}, pérdida de entrenamiento: {loss}\")\n",
    "            val_acc = test_oneshot(model, N_way, n_val)\n",
    "            print(f\"Precisión de validación en one-shot ({N_way}-way): {val_acc:.2f}%\")\n",
    "            if val_acc >= best_acc:\n",
    "                print(f\"Nueva mejor precisión: {val_acc:.2f}% (anterior: {best_acc:.2f}%)\")\n",
    "                model.save_weights(os.path.join(model_path, f'weights_fold_{fold}.h5'))\n",
    "                best_acc = val_acc\n",
    "\n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_acc = test_oneshot(model, N_way, n_val)\n",
    "    print(f\"Precisión en el conjunto de prueba para el pliegue {fold}: {test_acc:.2f}%\")\n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model):\n",
    "    pairs_test, targets_test = get_test_pairs()\n",
    "    predictions = model.predict(pairs_test)\n",
    "    predictions = predictions.ravel()\n",
    "\n",
    "    # Calcular métricas\n",
    "    acc = accuracy_score(targets_test, predictions > 0.5)\n",
    "    roc_auc = roc_auc_score(targets_test, predictions)\n",
    "\n",
    "    print(f\"Precisión en el conjunto de prueba: {acc:.2f}\")\n",
    "    print(f\"ROC-AUC en el conjunto de prueba: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando el pliegue 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/keras/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 100, pérdida de entrenamiento: 0.72700434923172\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2631927/3953790589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteración {i}, pérdida de entrenamiento: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_oneshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_way\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Precisión de validación en one-shot ({N_way}-way): {val_acc:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2631927/3953790589.py\u001b[0m in \u001b[0;36mtest_oneshot\u001b[0;34m(model, N_way, n_trials)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_oneshot_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_way\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2631927/3953790589.py\u001b[0m in \u001b[0;36mmake_oneshot_task\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_oneshot_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Función para crear una tarea de one-shot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_test_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtrue_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_test_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "fold=1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"\\nEntrenando el pliegue {fold}/{n_splits}\")\n",
    "\n",
    "    # Dividir los datos en entrenamiento y prueba para este pliegue\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Convertir los datos a DataFrames para mantener consistencia con el código original\n",
    "    x_train = pd.DataFrame(X_train, index=y_train)\n",
    "    x_test = pd.DataFrame(X_test, index=y_test)\n",
    "\n",
    "    # Crear diccionarios de índices por clase\n",
    "    def indices_save(labels):\n",
    "        class_dic = {}\n",
    "        for k in np.unique(labels):\n",
    "            indices = [i for i, label in enumerate(labels) if label == k]\n",
    "            class_dic[k] = indices\n",
    "        return class_dic\n",
    "\n",
    "    class_train_ind = indices_save(y_train)\n",
    "    class_test_ind = indices_save(y_test)\n",
    "\n",
    "    # Definir las funciones get_batch y make_oneshot_task utilizando los datos del pliegue actual\n",
    "\n",
    "    def get_batch(batch_size):\n",
    "        # Función para generar un lote de entrenamiento\n",
    "        categories = np.random.choice(list(class_train_ind.keys()), size=batch_size)\n",
    "        pairs = [np.zeros((batch_size, genes_len, 1)) for _ in range(2)]\n",
    "        targets = np.zeros((batch_size,))\n",
    "        targets[batch_size // 2:] = 1\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            category = categories[i]\n",
    "            idx_1 = np.random.choice(class_train_ind[category])\n",
    "            pairs[0][i, :, 0] = x_train.iloc[idx_1].values\n",
    "\n",
    "            if i >= batch_size // 2:\n",
    "                # Mismas clases\n",
    "                idx_2 = np.random.choice(class_train_ind[category])\n",
    "            else:\n",
    "                # Clases diferentes\n",
    "                category_2 = np.random.choice([k for k in class_train_ind.keys() if k != category])\n",
    "                idx_2 = np.random.choice(class_train_ind[category_2])\n",
    "\n",
    "            pairs[1][i, :, 0] = x_train.iloc[idx_2].values\n",
    "\n",
    "        return pairs, targets\n",
    "\n",
    "    def make_oneshot_task(N):\n",
    "        # Función para crear una tarea de one-shot\n",
    "        categories = np.random.choice(list(class_test_ind.keys()), size=N, replace=False)\n",
    "        true_category = categories[0]\n",
    "        indices = [np.random.choice(class_test_ind[cat]) for cat in categories]\n",
    "        test_image = x_test.iloc[indices[0]].values.reshape(1, genes_len, 1)\n",
    "        support_set = x_test.iloc[indices].values.reshape(N, genes_len, 1)\n",
    "\n",
    "        test_images = np.vstack([test_image] * N)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1  # La primera es la correcta\n",
    "\n",
    "        # Mezclar el soporte y las etiquetas\n",
    "        pairs = [test_images, support_set]\n",
    "        targets, pairs[0], pairs[1] = shuffle(targets, pairs[0], pairs[1])\n",
    "\n",
    "        return pairs, targets\n",
    "\n",
    "    def test_oneshot(model, N_way, n_trials):\n",
    "        n_correct = 0\n",
    "        for _ in range(n_trials):\n",
    "            inputs, targets = make_oneshot_task(N_way)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct += 1\n",
    "        return 100.0 * n_correct / n_trials\n",
    "\n",
    "    # Definir el modelo\n",
    "    model = get_siamese_model(input_shape)\n",
    "    optimizer = Adam(lr=0.000005)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    best_acc = -1  # Reiniciar la mejor precisión para cada pliegue\n",
    "    for i in range(1, n_iter + 1):\n",
    "        inputs, targets = get_batch(batch_size)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "\n",
    "        if i % evaluate_every == 0:\n",
    "            print(f\"Iteración {i}, pérdida de entrenamiento: {loss}\")\n",
    "            val_acc = test_oneshot(model, N_way, n_val)\n",
    "            print(f\"Precisión de validación en one-shot ({N_way}-way): {val_acc:.2f}%\")\n",
    "            if val_acc >= best_acc:\n",
    "                print(f\"Nueva mejor precisión: {val_acc:.2f}% (anterior: {best_acc:.2f}%)\")\n",
    "                model.save_weights(os.path.join(model_path, f'weights_fold_{fold}.h5'))\n",
    "                best_acc = val_acc\n",
    "\n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_acc = test_oneshot(model, N_way, n_val)\n",
    "    print(f\"Precisión en el conjunto de prueba para el pliegue {fold}: {test_acc:.2f}%\")\n",
    "\n",
    "    fold += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
